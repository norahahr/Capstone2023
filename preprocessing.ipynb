{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import pickle\n",
    "import re\n",
    "import spacy\n",
    "import string\n",
    "\n",
    "from csv import reader\n",
    "from ewiser.spacy.disambiguate import Disambiguator\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constant Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATHS: INFILES\n",
    "EWISER_CHECKPOINT = Path('./ewiser.semcor+wngt.pt')\n",
    "\n",
    "# Path to directory containing COCA text files\n",
    "COCA_DIR = Path(\"./COCA_text/text_2012-2015_ksr\")\n",
    "\n",
    "# Directorty to Elsevier OA text files\n",
    "ELSEVIER_OA_DIR = Path(\"./elsevier-oa/json\")\n",
    "\n",
    "# Path to file containing names of all files in ELSEVIER_OA_DIR\n",
    "ELSEVIER_OA_INDEX = Path(\"./elsevier-oa/os-ccby-40k-ids.csv\")\n",
    "\n",
    "# PATHS: OUTFILES\n",
    "\n",
    "# Path to file containing names of all COCA text files\n",
    "PREPROC_DIR = Path(\"./coca-preproc-spacy/\")\n",
    "\n",
    "# Path to outfile containing the filtered version of ELSEVIER Corpus\n",
    "ELSEVIER_DATA_PATH = Path(\"./raw_data.json\")\n",
    "\n",
    "# Path to directory where preprocessed data will be saved\n",
    "PREPROC_DIR = Path(\"./elsevier-preproc-spacy/\")\n",
    "\n",
    "# Path to file where all unique subject areas will be saved\n",
    "SUBJAREAS = Path(\"./subjareas.txt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-13 16:08:50 | INFO | pytorch_pretrained_bert.modeling | Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
      "2023-06-13 16:08:51 | INFO | pytorch_pretrained_bert.tokenization | loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-vocab.txt from cache at /Users/norahahr/.pytorch_pretrained_bert/cee054f6aafe5e2cf816d2228704e326446785f940f5451a5b26033516a4ac3d.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
      "2023-06-13 16:08:51 | INFO | pytorch_pretrained_bert.modeling | loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased.tar.gz from cache at /Users/norahahr/.pytorch_pretrained_bert/7fb0534b83c42daee7d3ddb0ebaa81387925b71665d6ea195c5447f1077454cd.eea60d9ebb03c75bb36302aa9d241d3b7a04bba39c360cf035e8bf8140816233\n",
      "2023-06-13 16:08:51 | INFO | pytorch_pretrained_bert.modeling | extracting archive file /Users/norahahr/.pytorch_pretrained_bert/7fb0534b83c42daee7d3ddb0ebaa81387925b71665d6ea195c5447f1077454cd.eea60d9ebb03c75bb36302aa9d241d3b7a04bba39c360cf035e8bf8140816233 to temp dir /var/folders/2m/v0bbm4tj7934__nywbb5p9ph0000gn/T/tmpss766u7b\n",
      "2023-06-13 16:08:59 | INFO | pytorch_pretrained_bert.modeling | Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "/opt/anaconda3/envs/ewiser/lib/python3.7/site-packages/torch/nn/modules/module.py:1433: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  \"Positional args are being deprecated, use kwargs instead. Refer to \"\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "wsd = Disambiguator(EWISER_CHECKPOINT, lang=\"en\")\n",
    "wsd.enable(nlp, 'wsd')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING: COCA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessing of the COCA is fairly stright forward. Particular sequences of noise are removed from the corpus and the texts are separated from each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_text(doc):\n",
    "    \"\"\"\n",
    "    Input path to file containing several texts from the COCA corpus. Removes noise and splits texts by common separator.\n",
    "    Returns list with cleaned texts.\n",
    "    \"\"\"\n",
    "    with open(doc, \"r\") as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    text = re.sub(r\"\\#|\\@ \\@ \\@ \\@ \\@ \\@ \\@ \\@ \\@ \\@\", \"\", text)\n",
    "    text = re.sub(r\"\\@\\@\\d{7}\", \"@@\", text)\n",
    "\n",
    "    texts = text.split(\"@@\")\n",
    "\n",
    "    return texts\n",
    "\n",
    "def coca_preprocess_doc(doc):\n",
    "    \"\"\"Input a text from the COCA corpus and preprocess it.\"\"\"\n",
    "\n",
    "    text_sentences = nltk.sent_tokenize(doc)\n",
    "    filtered_sentences = [sentence.translate(str.maketrans('', '', string.punctuation)) \n",
    "                          for sentence in text_sentences]\n",
    "    text_docs = list(nlp.pipe(filtered_sentences))\n",
    "\n",
    "\n",
    "    preproc_doc = {\n",
    "        'text_docs': text_docs\n",
    "    }\n",
    "\n",
    "    return preproc_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = ['mag', 'fic', 'news', 'spok']\n",
    "\n",
    "all_texts = []\n",
    "    \n",
    "for genre in genres:\n",
    "    filtered_texts = filter_text(f\"{COCA_DIR}/2015_{genre}.txt\")\n",
    "\n",
    "    for text in filtered_texts:\n",
    "        pp_text = coca_preprocess_doc(doc)\n",
    "        all_texts.append(pp_text)\n",
    "\n",
    "with open(f'{PREPROC_DIR}/2015.pickle', 'wb') as f:\n",
    "    pickle.dump(all_texts, f)\n",
    "            "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING: ELSEVIER OA CC-BY"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only the subject area and the textual data is of relevance for the analysis. Thus, all other data is filtered from the corpus. The full body text is also reconstructed with the original sentence order. All articles are stored with document ID, a list of subject areas, and full text as documents in a json-file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_obj = []\n",
    "\n",
    "with open(ELSEVIER_OA_INDEX) as id_file:\n",
    "    all_ids = reader(id_file)\n",
    "\n",
    "    for id in all_ids:\n",
    "        with open(f\"{ELSEVIER_OA_DIR}/{(id[0])}.json\") as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "            res = sorted(data[\"body_text\"], key=lambda x: x['startOffset'])\n",
    "            body_text = \" \".join([res[index][\"sentence\"] for index in range(len(res))])\n",
    "\n",
    "            article_data = {\"docID\": data.get(\"docId\", \"null\"),\n",
    "                            \"subjareas\": data[\"metadata\"].get(\"subjareas\", \"null\"),\n",
    "                            \"body_text\": body_text}\n",
    "\n",
    "            json_obj.append(article_data)\n",
    "\n",
    "with open(ELSEVIER_DATA_PATH, \"w\") as outfile:\n",
    "    json.dump(json_obj, outfile)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we find all academic disciplines that are contained within the corpus and we thereafter preprocess the docuemnts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ECON', 'DENT', 'ENGI', 'MULT', 'EART', 'ENVI', 'NEUR', 'NURS', 'MEDI', 'ENER', 'VETE', 'MATE', 'PHYS', 'MATH', 'SOCI', 'IMMU', 'DECI', 'PSYC', 'ARTS', 'AGRI', 'COMP', 'PHAR', 'CENG', 'HEAL', 'CHEM', 'BIOC', 'BUSI'}\n"
     ]
    }
   ],
   "source": [
    "unique_subjareas = set()\n",
    "\n",
    "with open(ELSEVIER_DATA_PATH, \"r\") as f:\n",
    "    elsevier_data = json.load(f)\n",
    "\n",
    "    for doc in elsevier_data:\n",
    "        subjareas = doc.get(\"subjareas\", [])\n",
    "        unique_subjareas.update(subjareas)\n",
    "\n",
    "# Print the unique subjareas\n",
    "print(unique_subjareas)\n",
    "\n",
    "with open(SUBJAREAS, \"w\") as f:\n",
    "    for subjarea in unique_subjareas:\n",
    "        f.write(\"%s\\n\" % subjarea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elsevier_preprocess_document(doc):\n",
    "    \"\"\"\n",
    "    Input a fail containing and article from the Elsevier OA CC-BY corpus. \n",
    "    Return a dictionary containing a preprocessed version of the body text, the document ID,\n",
    "    and the subject areas that the article belongs to. \n",
    "    \"\"\"\n",
    "    doc_id = doc['docID']\n",
    "    subj_areas = doc['subjareas']\n",
    "    body_text = doc['body_text']\n",
    "\n",
    "    text_sentences = nltk.sent_tokenize(body_text)\n",
    "    \n",
    "    filtered_sentences = [sentence.translate(str.maketrans('', '', string.punctuation)) \n",
    "                          for sentence in text_sentences]\n",
    "    text_docs = list(nlp.pipe(filtered_sentences))\n",
    "\n",
    "    preproc_doc = {\n",
    "        'docID': doc_id,\n",
    "        'subjareas': subj_areas,\n",
    "        'body_text_docs': text_docs\n",
    "    }\n",
    "\n",
    "    return preproc_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjareas = list(unique_subjareas)\n",
    "\n",
    "for subject in subjareas:\n",
    "    print(subject)\n",
    "    \n",
    "    subject_list = []\n",
    "\n",
    "    for doc in elsevier_data:\n",
    "        if subject in doc['subjareas']:\n",
    "            subject_list.append(doc)\n",
    "\n",
    "    subject_list = [elsevier_preprocess_document(doc) for doc in subject_list]\n",
    "\n",
    "    with open(f'{PREPROC_DIR}/{subject}.pickle', 'wb') as f:\n",
    "            pickle.dump(subject_list, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ewiser",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
