{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kullback Leibler Divergence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "from collections import Counter \n",
    "from pathlib import Path\n",
    "from scipy.special import kl_div"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constant Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATHS: INFILES\n",
    "\n",
    "# Path to directory containing preprocessed COCA files\n",
    "COCA_PREPROC_DIR = Path(\"./coca-preproc-spacy/\")\n",
    "\n",
    "# Path to directory containing preprocessed Elsevier files\n",
    "ELSEVIER_PREPROC_DIR = Path(\"./elsevier-preproc-spacy/\")\n",
    "\n",
    "# Path to file containing all subject areas\n",
    "SUBJAREAS = Path(\"./subjareas.txt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load COCA\n",
    "Since the COCA data will be used throughout the entire process, it will be loaded now for ease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7802\n"
     ]
    }
   ],
   "source": [
    "with open(f'{COCA_PREPROC_DIR}/2015.pickle', 'rb') as f:\n",
    "    coca = pickle.load(f)\n",
    "\n",
    "print(len(coca))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating KLD\n",
    "In the following section all functions necessary to conduct a KLD analysis for both token unigrams and POS trigrams are defined. We also define a function to conduct bootstrapping. Since stopwords are excluded during the token unigram analysis, we also load all stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stopwods to filter out when counting token frequency distributions\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stopwords = nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_freq_dist(text_sample, text_key, min_occ = 10):\n",
    "    \"\"\"\n",
    "    Calculates the frequency distribution of tokens within a text sample.\n",
    "    \n",
    "    Args:\n",
    "    text_sample: A list of preprocessed texts.\n",
    "    text_key: A string describing the key for where body text is stored for each article object. \n",
    "\n",
    "    Returns: A dictionary with each token as key and frequency as value. \n",
    "    \"\"\"\n",
    "    all_tokens = []\n",
    "\n",
    "    for text in text_sample:\n",
    "        text_tokens = [token.text for sentence in text[text_key] for token in sentence]\n",
    "        filter_tokens = [token.lower() for token in text_tokens if token not in stopwords]\n",
    "\n",
    "        all_tokens.extend(filter_tokens)\n",
    "\n",
    "    tokens_counts = dict(Counter(all_tokens))\n",
    "    filtered_counts = [\n",
    "        (token, count) for token, count in tokens_counts.items() if count >= min_occ\n",
    "        ]\n",
    "\n",
    "    return filtered_counts\n",
    "\n",
    "def pos_trigram_freq_dist(text_sample, text_key, min_occ = 10):\n",
    "    \"\"\"Calculates the frequency distribution of pos trigrams within a text sample.\n",
    "    \n",
    "    Args:\n",
    "    text_sample: A list of preprocessed texts.\n",
    "    text_key: A string describing the key for where body text is stored for each \n",
    "    article object. \n",
    "\n",
    "    Returns: A dictionary with each trigram as key and frequency as value. \n",
    "    \"\"\"\n",
    "\n",
    "    all_trigrams = []\n",
    " \n",
    "    for text in text_sample:\n",
    "        pos_tags = [[token.tag_ for token in sentence] for sentence in text[text_key]]\n",
    "        pos_trigrams = ['-'.join(sentence[token:token+3]) \n",
    "                        for sentence in pos_tags \n",
    "                        for token in range(len(sentence)-2)]\n",
    "\n",
    "        all_trigrams.extend(pos_trigrams)\n",
    "\n",
    "    trigram_counts = dict(Counter(all_trigrams))\n",
    "    filtered_counts = [\n",
    "        (trigram, count) for trigram, count in trigram_counts.items() if count >= min_occ\n",
    "        ]\n",
    "\n",
    "    return filtered_counts\n",
    "\n",
    "def kld(p, q):\n",
    "    \"Calculate the Kullback-Leibler Divergence between two frequency distributions p and q.\"\n",
    "\n",
    "    p_keys = [item[0] for item in p]\n",
    "    q_keys = [item[0] for item in q]\n",
    "\n",
    "    tokens = list(set(p_keys).union(set(q_keys)))\n",
    "\n",
    "    # Ensure the same items in both lists and the same order\n",
    "    p_ordered = [\n",
    "        (k, [t[1] for t in p if t[0] == k][0]) if k in p_keys else (k, 0) for k in tokens\n",
    "        ]\n",
    "    q_ordered = [\n",
    "        (k, [t[1] for t in q if t[0] == k][0]) if k in q_keys else (k, 0) for k in tokens\n",
    "        ]\n",
    "    \n",
    "    p_ordered_values = np.array([item[1] for item in p_ordered])\n",
    "    q_ordered_values = np.array([item[1] for item in q_ordered])\n",
    "\n",
    "    # Normalize the arrays to obtain probability distributions\n",
    "    p_dist = p_ordered_values / p_ordered_values.sum()\n",
    "    q_dist = q_ordered_values / q_ordered_values.sum()\n",
    "\n",
    "    pointwise_kld = {}\n",
    "\n",
    "    for i in range(len(p_ordered_values)):\n",
    "        if q_dist[i] == 0:\n",
    "            pointwise_kld[q_ordered[i][0]] = kl_div(p_dist[i], np.exp(-9))\n",
    "        else:\n",
    "            pointwise_kld[q_ordered[i][0]]= kl_div(p_dist[i], q_dist[i])\n",
    "\n",
    "    kld = np.sum(list(pointwise_kld.values()))\n",
    "    \n",
    "    return kld, pointwise_kld\n",
    "\n",
    "def bootstrap_kld_ci(\n",
    "        baseline_text_dist, \n",
    "        comparison_texts, \n",
    "        num_resamples=1000, \n",
    "        alpha=0.05, \n",
    "        versus = 0,\n",
    "        type = \"token\"\n",
    "        ):\n",
    "    \"\"\"\n",
    "    Calculates KLD for two corpora, with bootstrapping one one the distributions.\n",
    "    It calculates KLD for either token distribution of POS trigram distributions.\n",
    "\n",
    "    Args:\n",
    "    baseline_text_dist: A frequency distribution of either tokens of POS trigrams\n",
    "    (depending on 'type'), for which the resampled corpora will be compared to. \n",
    "    comparison_texts: A corpora containing preprocessed text documents that will be \n",
    "    bootstrapped.\n",
    "    num_resamples: The number of iterations for bootstrappign. Default = 1000.\n",
    "    alpha: The alpha value for which to calculate the confidence intervals. Default = 0.05.\n",
    "    verus: Determines whether to treat baseline_text_dist as the dependent or independent \n",
    "    corpus in the calculation. Defaul = 0, indicating comparison texts vs. baseline texts.\n",
    "    type: Determine whether to calculate KLD for tokens or POS trigrams. Default = \"token\".\n",
    "    \"\"\"\n",
    "    kl_divergences = []\n",
    "    n = len(comparison_texts)\n",
    "\n",
    "    for i in range(num_resamples):\n",
    "        resampled_texts = np.random.choice(comparison_texts, size=n, replace=True)\n",
    "\n",
    "        if type == \"token\":\n",
    "            resampled_dist = token_freq_dist(resampled_texts, \"body_text_docs\")\n",
    "        else: \n",
    "            resampled_dist = pos_trigram_freq_dist(resampled_texts, \"body_text_docs\")\n",
    "        \n",
    "        if versus == 0:\n",
    "            kl_divergence_i = kld(resampled_dist, baseline_text_dist)\n",
    "        else:\n",
    "            kl_divergence_i = kld(baseline_text_dist, resampled_dist)\n",
    "\n",
    "        kl_divergences.append(kl_divergence_i[0])\n",
    "\n",
    "    average_kld = np.mean(kl_divergences)    \n",
    "    ci_lower = np.percentile(kl_divergences, alpha/2 * 100)\n",
    "    ci_upper = np.percentile(kl_divergences, (1 - alpha/2) * 100)\n",
    "\n",
    "    return average_kld, ci_lower, ci_upper"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KLD for Token Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataframes to store results\n",
    "subj_coca_token_df = pd.DataFrame(columns=['subj', 'sum', 'lower CI', 'upper CI', 'pointwise KLD'])\n",
    "coca_subj_token_df = pd.DataFrame(columns=['subj', 'sum', 'lower CI', 'upper CI', 'pointwise KLD'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate token frequency distribution in COCA\n",
    "coca_token_dist = token_freq_dist(coca, 'text_docs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42896"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of unique tokens in filtered COCA.\n",
    "len(coca_token_dist)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run through each academic discipline in the Elsevier OA CC-BY corpus for which the pointwise KLD is calculated for the entire corpus and bootstrapping yields 95% confidence intervals and an estimated KLD average for the corpus. The KLD is calculated in both directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{SUBJAREAS}', 'r') as subj_list_file:\n",
    "    for subject in subj_list_file:\n",
    "        subject = subject.strip()\n",
    "        print(subject)\n",
    "\n",
    "        with open(f'{ELSEVIER_PREPROC_DIR}/{subject}.pickle', 'rb') as articles_file:\n",
    "            articles = pickle.load(articles_file)\n",
    "        \n",
    "        subj_token_dist = token_freq_dist(articles, 'body_text_docs')\n",
    "\n",
    "        # Subject vs. Coca\n",
    "        bootstrap_subj_coca = bootstrap_kld_ci(coca_token_dist, articles, num_resamples= 1, alpha=0.05, versus = 0)\n",
    "        pointwise_subj_coca = kld(subj_token_dist, coca_token_dist)[1]\n",
    "        \n",
    "        subj_coca_row = {\n",
    "            'subj': subject, 'sum': bootstrap_subj_coca[0], 'lower CI': bootstrap_subj_coca[1], \n",
    "            'upper CI': bootstrap_subj_coca[2], 'pointwise KLD': pointwise_subj_coca\n",
    "            }\n",
    "        \n",
    "        subj_coca_token_df = subj_coca_token_df.append(subj_coca_row, ignore_index=True)\n",
    "\n",
    "        # Coca vs. Subject\n",
    "        bootstrap_coca_subj = bootstrap_kld_ci(coca_token_dist, articles, num_resamples= 1, alpha=0.05, versus = 1)\n",
    "        pointwise_coca_subj = kld(subj_token_dist, coca_token_dist)[1]\n",
    "\n",
    "        coca_subj_row = {\n",
    "            'subj': subject, 'sum': bootstrap_coca_subj[0], 'lower CI': bootstrap_coca_subj[1], \n",
    "            'upper CI': bootstrap_coca_subj[2], 'pointwise KLD': pointwise_coca_subj\n",
    "            }\n",
    "        \n",
    "        coca_subj_token_df = coca_subj_token_df.append(coca_subj_row, ignore_index=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KLD for POS Trigrams\n",
    "Once again, we first count the POS-trigram distributions. Then we calculate the KLD and associated confidence intervalss per subject though bootstrapping. Pointwise KLD is also calculated using the entire corpus of each academic discipline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataframes to store results\n",
    "subj_coca_pos_df = pd.DataFrame(columns=['subj', 'sum', 'lower CI', 'upper CI', 'pointwise KLD'])\n",
    "coca_subj_pos_df = pd.DataFrame(columns=['subj', 'sum', 'lower CI', 'upper CI', 'pointwise KLD'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "coca_pos_dist = pos_trigram_freq_dist(coca, 'text_docs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{SUBJAREAS}', 'r') as subj_list_file:\n",
    "    for subject in subj_list_file:\n",
    "        subject = subject.strip()\n",
    "        print(subject)\n",
    "\n",
    "        with open(f'{ELSEVIER_PREPROC_DIR}/{subject}.pickle', 'rb') as articles_file:\n",
    "            articles = pickle.load(articles_file)\n",
    "        \n",
    "        subj_pos_dist = pos_trigram_freq_dist(articles, 'body_text_docs')\n",
    "\n",
    "        # Subject vs. Coca\n",
    "        subj_coca_kld = kld(subj_pos_dist, coca_pos_dist)\n",
    "        print(f'subject_coca_kld is:{subj_coca_kld[0]}')\n",
    "        subj_coca_ci = bootstrap_kld_ci(coca_token_dist, articles, num_resamples= 1000, alpha=0.05, versus = 0)\n",
    "        \n",
    "        subj_coca_row = {\n",
    "            'subj': subject, 'sum': bootstrap_subj_coca[0], 'lower CI': bootstrap_subj_coca[1], \n",
    "            'upper CI': bootstrap_subj_coca[2], 'pointwise KLD': pointwise_subj_coca\n",
    "            }\n",
    "        subj_coca_pos_df = subj_coca_pos_df.append(subj_coca_row, ignore_index=True)\n",
    "\n",
    "        # Coca vs. Subject\n",
    "        coca_subj_kld = kld(coca_pos_dist, subj_pos_dist)\n",
    "        print(f'coca_subject_kld is {coca_subj_kld[0]}')\n",
    "        coca_subj_ci = bootstrap_kld_ci(coca_token_dist, articles, num_resamples= 1000, alpha=0.05, versus = 1)\n",
    "\n",
    "        coca_subj_row = {\n",
    "            'subj': subject, 'sum': bootstrap_coca_subj[0], 'lower CI': bootstrap_coca_subj[1], \n",
    "            'upper CI': bootstrap_coca_subj[2], 'pointwise KLD': pointwise_coca_subj\n",
    "            }\n",
    "        coca_subj_pos_df = coca_subj_pos_df.append(coca_subj_row, ignore_index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ewiser",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
