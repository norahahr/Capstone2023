{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import spacy\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from pathlib import Path\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constant paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to directory containing preprocessed COCA files\n",
    "COCA_PREPROC_DIR = Path(\"./coca-preproc-spacy/\")\n",
    "\n",
    "# Path to directory containing preprocessed Elsevier files\n",
    "ELSEVIER_PREPROC_DIR = Path(\"./elsevier-preproc-spacy/\")\n",
    "\n",
    "# Path to file containing all subject areas\n",
    "SUBJAREAS = Path(\"./subjareas.txt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Comparison Words\n",
    "First, we select 1000 tokens that are present in all corpora. These will be used to measure the distance between the general language corpus and the academic corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stopwords = nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tokens = set()\n",
    "\n",
    "with open(f'{COCA_PREPROC_DIR}/2015.pickle', 'rb') as f:\n",
    "    coca = pickle.load(f)\n",
    "\n",
    "    for text in coca:\n",
    "        text_tokens = [\n",
    "            token.text for sentence in text['text_docs'] for token in sentence\n",
    "            ]\n",
    "        filter_tokens = [\n",
    "            token.lower() for token in text_tokens if token.lower() not in stopwords\n",
    "            ]\n",
    "\n",
    "        unique_tokens.update(filter_tokens)\n",
    "\n",
    "with open(f'{SUBJAREAS}', 'r') as subj_list_file:\n",
    "    for subject in subj_list_file:\n",
    "        subject = subject.strip()\n",
    "\n",
    "        with open(f'{ELSEVIER_PREPROC_DIR}/{subject}.pickle', 'rb') as articles_file:\n",
    "            articles = pickle.load(articles_file)\n",
    "\n",
    "        unique_subj_tokens = set()\n",
    "\n",
    "        for text in articles:\n",
    "            text_tokens = [\n",
    "                token.text for sentence in text['body_text_docs'] for token in sentence if token.text\n",
    "                ]\n",
    "            filter_tokens = [\n",
    "                token.lower() for token in text_tokens if token.lower() not in stopwords\n",
    "                ]\n",
    "\n",
    "            unique_subj_tokens.update(filter_tokens)\n",
    "        \n",
    "        unique_tokens = unique_tokens.intersection(unique_subj_tokens)\n",
    "\n",
    "tokens_1000 = random.sample(unique_tokens, k=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['demographic',\n",
       " 'database',\n",
       " 'discernible',\n",
       " 'sufficiently',\n",
       " 'nonetheless',\n",
       " 'assuming',\n",
       " 'irrelevant',\n",
       " 'let',\n",
       " 'ongoing',\n",
       " 'abstraction',\n",
       " 'landmarks',\n",
       " 'augmented',\n",
       " 'shear',\n",
       " 'helped',\n",
       " 'trends']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_1000 = random.sample(unique_tokens, k=1000)\n",
    "tokens_1000[:15]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COCA Embedding Space\n",
    "First we create an embedding space based on COCA, and save all the word vectors from the 1000 randomly selected tokens in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "coca_sentences = [\n",
    "    [token.text.lower() for token in sentence if token.text.lower() not in stopwords] \n",
    "    for text in coca for sentence in text['text_docs']\n",
    "    ]\n",
    "\n",
    "coca_model = Word2Vec(coca_sentences, window=5, min_count=1, workers=4)\n",
    "coca_token_vectors = [(token, coca_model.wv[token]) for token in tokens_1000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('demographic',\n",
       "  array([-0.44434673,  0.16190062, -0.24173903, -0.36417148, -0.20352486,\n",
       "         -0.37053123,  0.4803917 ,  0.26165414, -0.3368373 ,  0.23189664,\n",
       "         -0.02670773, -0.24587852, -0.09785329, -0.1316972 , -0.15303685,\n",
       "          0.26428905, -0.24956234,  0.22814009, -0.12520492,  0.03824827,\n",
       "          0.59004533, -0.02376954,  0.61472243,  0.87467647, -0.39327505,\n",
       "          0.30194232,  0.3666166 , -0.10386968,  0.4176317 ,  0.16066249,\n",
       "         -0.19098926, -0.21286441,  0.31128606, -0.16328265,  0.6271264 ,\n",
       "         -0.33382308,  0.22877516, -0.49367693, -0.58774924, -0.11712056,\n",
       "         -0.08501099,  0.53369397, -0.1648046 ,  0.19690768,  0.37831956,\n",
       "          0.36941496, -0.8091688 ,  0.75390965,  0.45207468, -0.01725184,\n",
       "         -0.6082515 ,  0.23144749, -0.46796167, -0.5329031 ,  0.34345186,\n",
       "          0.21273479,  0.6173254 , -0.36346298, -0.18090485,  0.4016873 ,\n",
       "         -0.2611292 , -0.15879384,  0.11894826,  0.6381067 , -0.22387555,\n",
       "         -0.6162295 , -0.08493779, -0.03818796, -0.41119057,  0.23716708,\n",
       "         -0.00925748, -0.35993594,  0.23678458, -0.2772247 ,  0.510806  ,\n",
       "          0.24738286,  0.34791055, -0.34214443,  0.01679944, -0.53575784,\n",
       "         -0.06290177,  0.27886596,  0.6621258 ,  0.40519398, -0.5067554 ,\n",
       "          0.1539318 ,  0.69478357,  0.970284  , -0.18596381, -0.13568658,\n",
       "          0.2882599 , -0.03688863,  0.541555  , -0.10866971, -0.7748007 ,\n",
       "         -0.34531385, -0.00742095, -0.34432393,  0.03024632,  0.4383009 ],\n",
       "        dtype=float32)),\n",
       " ('database',\n",
       "  array([-1.2906076 ,  0.03711159, -0.5191537 , -0.26016894, -0.15509748,\n",
       "          0.24781807,  0.40818322,  0.49458614, -0.4456954 ,  0.06289012,\n",
       "         -0.5690855 , -1.2181684 , -0.19305536, -0.92708385,  0.79401845,\n",
       "         -0.14511557,  0.4137431 , -0.14056353, -0.4648746 , -0.09688468,\n",
       "          0.13114296,  1.1845369 ,  0.12118357,  0.32245165,  0.24717613,\n",
       "         -0.2791612 ,  0.23088384,  0.6271424 , -0.16735585, -0.1617946 ,\n",
       "         -0.29808518, -0.2069592 ,  0.29663178, -0.6363715 ,  0.6718941 ,\n",
       "         -0.36706743,  0.48735425, -0.1629163 , -0.1065114 , -1.3175277 ,\n",
       "         -0.7006917 ,  0.5469777 , -0.38229525,  0.39929184,  0.28905484,\n",
       "         -0.5180907 , -0.50171924,  0.7974554 , -0.20807944, -0.04158899,\n",
       "         -0.9008612 ,  0.09526581,  0.19387698, -0.73634905,  0.10067708,\n",
       "         -0.6966228 ,  0.72256607, -0.05091668, -0.30518642, -0.3325229 ,\n",
       "         -0.12290455,  0.5947163 , -0.22857027,  0.8851918 , -0.455017  ,\n",
       "         -0.10369438, -0.07405529, -0.15529852,  0.30282205,  0.17007872,\n",
       "         -0.31935272, -0.30344254, -0.2988766 , -0.5046425 ,  0.32089347,\n",
       "          0.6930486 ,  0.21539634, -0.310641  ,  0.2510939 , -0.30566847,\n",
       "         -0.12032034,  0.11584794,  1.2552202 ,  0.42693552,  0.06837871,\n",
       "         -0.04746255, -0.39946327,  1.1781284 , -1.243285  , -0.44589674,\n",
       "          0.0443781 ,  0.09286491,  0.3066023 ,  0.13453807, -0.2599441 ,\n",
       "          0.41646296, -0.8234767 ,  0.42038038, -0.48917612,  0.1734667 ],\n",
       "        dtype=float32)),\n",
       " ('discernible',\n",
       "  array([-0.21092993,  0.2309259 , -0.23503998, -0.13926873, -0.06011195,\n",
       "         -0.19873965,  0.32308337,  0.26639506, -0.17083788, -0.15641716,\n",
       "         -0.05886027, -0.1558911 , -0.16837674, -0.08168742,  0.08122322,\n",
       "         -0.04866279,  0.09072156,  0.01307312,  0.04019903, -0.14672664,\n",
       "          0.11859591,  0.05743829,  0.09114853, -0.03149461,  0.0582785 ,\n",
       "          0.12614384,  0.08224571,  0.01739575, -0.00706921, -0.03588623,\n",
       "          0.02190612, -0.17306513,  0.04369639, -0.18256982,  0.23834597,\n",
       "          0.06219482,  0.12259685, -0.18391013, -0.15176062, -0.2605512 ,\n",
       "         -0.06251833, -0.16976732, -0.36917534, -0.0750188 ,  0.15783198,\n",
       "          0.02437443, -0.23050196,  0.04678201,  0.05789572,  0.04507736,\n",
       "         -0.26968202, -0.14637612, -0.11462303, -0.11259116,  0.10865711,\n",
       "         -0.01326372,  0.40075484, -0.15020399, -0.07050315,  0.02554414,\n",
       "         -0.07271748,  0.26165864, -0.00533679,  0.11604139, -0.14160635,\n",
       "          0.1218823 ,  0.05682554, -0.01918286, -0.18842702,  0.18278359,\n",
       "         -0.29850128, -0.01720325,  0.11074094,  0.01324017,  0.23987685,\n",
       "          0.21326783, -0.07780414,  0.04770805, -0.10383619, -0.04661662,\n",
       "         -0.10586374,  0.12568079,  0.03426919,  0.09097796,  0.02733929,\n",
       "         -0.10797974,  0.08675443,  0.31902862,  0.06276763,  0.06361867,\n",
       "          0.38363817,  0.01394749,  0.1252218 , -0.1020432 ,  0.12193453,\n",
       "         -0.10235609,  0.11481097, -0.16664712, -0.12154231,  0.24758995],\n",
       "        dtype=float32)),\n",
       " ('sufficiently',\n",
       "  array([-0.25493467,  0.14415564, -0.03412683, -0.21524423,  0.06453253,\n",
       "         -0.05359498,  0.1456488 ,  0.19591032, -0.31090736, -0.05498819,\n",
       "         -0.0662979 , -0.43310872, -0.30084264, -0.19539791,  0.25895986,\n",
       "         -0.02081408, -0.13784263,  0.12314543, -0.16865145, -0.11990399,\n",
       "          0.27409613,  0.2536377 ,  0.4001958 ,  0.14594933,  0.1162193 ,\n",
       "          0.27043328,  0.13794436,  0.0350279 , -0.05993734,  0.15941937,\n",
       "         -0.17021255, -0.21739066,  0.18054447, -0.4376504 ,  0.2760688 ,\n",
       "         -0.08576357,  0.1664645 , -0.20012316, -0.23045671, -0.3986684 ,\n",
       "          0.02541592, -0.18153302, -0.30269012,  0.100714  ,  0.2528076 ,\n",
       "          0.04672432, -0.52830136,  0.09749843,  0.09897446, -0.15410879,\n",
       "         -0.37414572, -0.20996967, -0.280538  , -0.33583644,  0.13080832,\n",
       "          0.07806399,  0.3936005 , -0.24887815, -0.13735276,  0.28249002,\n",
       "          0.09093533,  0.1256493 , -0.23001859,  0.19522126, -0.08678079,\n",
       "          0.01485934, -0.04660204, -0.1752509 , -0.21753955,  0.39880124,\n",
       "         -0.27807942, -0.34942132,  0.09772325,  0.03175781,  0.2012029 ,\n",
       "          0.33494878, -0.11470962, -0.04423676, -0.08200923,  0.01511916,\n",
       "         -0.0570647 ,  0.08566346,  0.20455997,  0.21559526, -0.30052468,\n",
       "         -0.05166688,  0.1252623 ,  0.3855745 ,  0.03413394,  0.10199422,\n",
       "          0.41258708, -0.15471883,  0.18553211, -0.24874759,  0.13560435,\n",
       "         -0.10839839,  0.20867135, -0.3146659 , -0.36781475,  0.43752897],\n",
       "        dtype=float32)),\n",
       " ('nonetheless',\n",
       "  array([-0.2867072 ,  0.25699848, -0.22972861, -0.2837353 , -0.17115472,\n",
       "         -0.34566683,  0.44993994,  0.37121838, -0.43501392, -0.16500486,\n",
       "          0.09014572, -0.4956982 , -0.37838438, -0.12091493, -0.10735412,\n",
       "          0.34935898,  0.00332864,  0.13434155, -0.24873151, -0.06912313,\n",
       "          0.9829685 ,  0.4438449 ,  0.66288346,  0.17296582,  0.09983563,\n",
       "          0.7625724 ,  0.35249227, -0.35526216, -0.4271284 ,  0.29965517,\n",
       "         -0.23311126, -0.18988088,  0.38116628, -0.6166102 ,  0.7064242 ,\n",
       "         -0.21782349,  0.17515783, -0.22928652, -0.13954905, -0.17461105,\n",
       "          0.14129068, -0.04353954, -0.3915836 ,  0.1404657 ,  0.31143045,\n",
       "          0.00643912, -0.80363923,  0.34129402,  0.2840555 , -0.29663935,\n",
       "         -0.5048979 , -0.10257488, -0.4386725 , -0.6368134 ,  0.40048146,\n",
       "          0.01177324,  0.27135098, -0.14932422, -0.25344053,  0.50828266,\n",
       "          0.14394398,  0.13229159, -0.25275698,  0.60793376, -0.11013544,\n",
       "         -0.03948632,  0.08071077, -0.08324535, -0.23668899,  0.5568697 ,\n",
       "         -0.2664356 , -0.69422966,  0.12471353, -0.22878861,  0.2356669 ,\n",
       "          0.27496403, -0.12783536, -0.03772591, -0.14697988,  0.3299331 ,\n",
       "         -0.10191644, -0.0163159 ,  0.34872735, -0.0281227 , -0.18927261,\n",
       "          0.06827439,  0.52013373,  0.48456997, -0.10250334,  0.13749634,\n",
       "          0.55190086, -0.40930444,  0.22231214, -0.19080311, -0.174438  ,\n",
       "         -0.03752294,  0.39864486, -0.29581755, -0.29556355,  0.806485  ],\n",
       "        dtype=float32))]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coca_token_vectors[:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ElSevier OA CC-BY Embedding Space\n",
    "Next, we create an embedding space for each academic corpus and compare the word vectors in the academic corpora with the word vectors in the COCA embedding space. This procedure is bootstrapped with 100 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subj_vector_sim(articles, vector_token_list):\n",
    "    \"\"\"\n",
    "    Given a set of articles and a list of word vectors, create a word2vec embedding space\n",
    "    based on articles and return the average cosine similarity between each word vector in \n",
    "    vector_token_list and their word vector in the new embedding space.\n",
    "\n",
    "    Args:\n",
    "    articles: Preprocessed articles from a specific discipline in the \n",
    "    ElSevier OA CC-BY corpus.\n",
    "    vector_token_list: A list of tuples. The item in the tuple contains a token, and the second \n",
    "    item in contains the word vector for that token in the COCA embedding space.\n",
    "    \"\"\"\n",
    "\n",
    "    subj_sentences = [\n",
    "        [token.text.lower() for token in sentence if token.text.lower() not in stopwords] \n",
    "        for text in articles for sentence in text['body_text_docs']\n",
    "        ]\n",
    "\n",
    "    subj_model = Word2Vec(subj_sentences, window=5, min_count=1, workers=4)\n",
    "\n",
    "    cosine_similarities = [\n",
    "        cosine_similarity(subj_model[token[0]], token[1]) for token in vector_token_list\n",
    "        ]\n",
    "\n",
    "    avg_cosine_similarity = np.mean(cosine_similarities)\n",
    "\n",
    "    return avg_cosine_similarity\n",
    "\n",
    "def bootstrap_word_embed_ci(\n",
    "        subj_articles,\n",
    "        word_vector_token_list, \n",
    "        num_resamples=100, \n",
    "        alpha=0.05, \n",
    "        ):\n",
    "    \"\"\"\n",
    "    Bootstrap the vector cosine similarity procedure to generate an average cosine similarity\n",
    "    with confidence intervals.\n",
    "\n",
    "    Args:\n",
    "    subj_articles: Preprocessed articles from a specific discipline in the \n",
    "    ElSevier OA CC-BY corpus.\n",
    "    word_vector_token_list: A list of tuples. The item in the tuple contains a token, and the second \n",
    "    item in contains the word vector for that token in the COCA embedding space.\n",
    "    num_resamples: The number of iterations for bootstrappign. Default = 100.\n",
    "    alpha: The alpha value for which to calculate the confidence intervals. Default = 0.05.\n",
    "    \"\"\"\n",
    "    average_cosine_sim = []\n",
    "    n = len(subj_articles)\n",
    "\n",
    "    for i in range(num_resamples):\n",
    "        resampled_texts = np.random.choice(subj_articles, size=n, replace=True)\n",
    "\n",
    "        sample_vector_sim = subj_vector_sim(resampled_texts, word_vector_token_list)\n",
    "\n",
    "        average_cosine_sim.append(sample_vector_sim)\n",
    "\n",
    "    average_sim = np.mean(average_cosine_sim)    \n",
    "    ci_lower = np.percentile(average_cosine_sim, alpha/2 * 100)\n",
    "    ci_upper = np.percentile(average_cosine_sim, (1 - alpha/2) * 100)\n",
    "\n",
    "    return average_sim, ci_lower, ci_upper\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe to save results\n",
    "avg_cosine_sim_df = pd.DataFrame(columns=['subj', 'avg', 'lower CI', 'upper CI'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{SUBJAREAS}', 'r') as subj_list_file:\n",
    "    for subject in subj_list_file:\n",
    "        subject = subject.strip()\n",
    "        print(subject)\n",
    "\n",
    "        with open(f'{ELSEVIER_PREPROC_DIR}/{subject}.pickle', 'rb') as articles_file:\n",
    "            articles = pickle.load(articles_file)\n",
    "\n",
    "            bootstrap_word_vec_sim = bootstrap_word_embed_ci(articles, coca_token_vectors)\n",
    "            subject_row = {\n",
    "            'subj': subject, 'avg': bootstrap_word_vec_sim[0], \n",
    "            'lower CI': bootstrap_word_vec_sim[1], 'upper CI': bootstrap_word_vec_sim[2]\n",
    "            }\n",
    "            \n",
    "            avg_cosine_sim_df.append(subject_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subj</th>\n",
       "      <th>avg</th>\n",
       "      <th>lower CI</th>\n",
       "      <th>upper CI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEM</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.973609</td>\n",
       "      <td>0.976081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CENG</td>\n",
       "      <td>0.975214</td>\n",
       "      <td>0.974769</td>\n",
       "      <td>0.976625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DENT</td>\n",
       "      <td>0.975412</td>\n",
       "      <td>0.975137</td>\n",
       "      <td>0.976084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PHYS</td>\n",
       "      <td>0.976533</td>\n",
       "      <td>0.976112</td>\n",
       "      <td>0.977410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PHAR</td>\n",
       "      <td>0.977424</td>\n",
       "      <td>0.976196</td>\n",
       "      <td>0.977934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AGRI</td>\n",
       "      <td>0.977696</td>\n",
       "      <td>0.977221</td>\n",
       "      <td>0.978083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>IMMU</td>\n",
       "      <td>0.978124</td>\n",
       "      <td>0.977284</td>\n",
       "      <td>0.979814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ENVI</td>\n",
       "      <td>0.979977</td>\n",
       "      <td>0.979750</td>\n",
       "      <td>0.981548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>BIOC</td>\n",
       "      <td>0.980124</td>\n",
       "      <td>0.978264</td>\n",
       "      <td>0.981100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NEUR</td>\n",
       "      <td>0.982154</td>\n",
       "      <td>0.980366</td>\n",
       "      <td>0.983440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>EART</td>\n",
       "      <td>0.984368</td>\n",
       "      <td>0.983190</td>\n",
       "      <td>0.985405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>MATE</td>\n",
       "      <td>0.986341</td>\n",
       "      <td>0.986323</td>\n",
       "      <td>0.987721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ENER</td>\n",
       "      <td>0.986897</td>\n",
       "      <td>0.985649</td>\n",
       "      <td>0.987831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NURS</td>\n",
       "      <td>0.988235</td>\n",
       "      <td>0.987644</td>\n",
       "      <td>0.989873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>MEDI</td>\n",
       "      <td>0.989642</td>\n",
       "      <td>0.988009</td>\n",
       "      <td>0.990804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>DECI</td>\n",
       "      <td>0.989680</td>\n",
       "      <td>0.989050</td>\n",
       "      <td>0.991486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>VETE</td>\n",
       "      <td>0.992349</td>\n",
       "      <td>0.992143</td>\n",
       "      <td>0.993233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ENGI</td>\n",
       "      <td>0.993466</td>\n",
       "      <td>0.992159</td>\n",
       "      <td>0.994520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>MATH</td>\n",
       "      <td>0.995436</td>\n",
       "      <td>0.994590</td>\n",
       "      <td>0.996427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>HEAL</td>\n",
       "      <td>0.996246</td>\n",
       "      <td>0.994962</td>\n",
       "      <td>0.997819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>COMP</td>\n",
       "      <td>0.996544</td>\n",
       "      <td>0.996474</td>\n",
       "      <td>0.996676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>PSYC</td>\n",
       "      <td>0.996987</td>\n",
       "      <td>0.995906</td>\n",
       "      <td>0.998350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ECON</td>\n",
       "      <td>0.997437</td>\n",
       "      <td>0.996035</td>\n",
       "      <td>0.998860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>BUSI</td>\n",
       "      <td>0.997753</td>\n",
       "      <td>0.997017</td>\n",
       "      <td>0.999170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ARTS</td>\n",
       "      <td>0.997935</td>\n",
       "      <td>0.997172</td>\n",
       "      <td>0.999480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>SOCI</td>\n",
       "      <td>0.997998</td>\n",
       "      <td>0.997556</td>\n",
       "      <td>0.999043</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    subj       avg  lower CI  upper CI\n",
       "0   CHEM  0.975000  0.973609  0.976081\n",
       "1   CENG  0.975214  0.974769  0.976625\n",
       "2   DENT  0.975412  0.975137  0.976084\n",
       "3   PHYS  0.976533  0.976112  0.977410\n",
       "4   PHAR  0.977424  0.976196  0.977934\n",
       "5   AGRI  0.977696  0.977221  0.978083\n",
       "6   IMMU  0.978124  0.977284  0.979814\n",
       "7   ENVI  0.979977  0.979750  0.981548\n",
       "8   BIOC  0.980124  0.978264  0.981100\n",
       "9   NEUR  0.982154  0.980366  0.983440\n",
       "10  EART  0.984368  0.983190  0.985405\n",
       "11  MATE  0.986341  0.986323  0.987721\n",
       "12  ENER  0.986897  0.985649  0.987831\n",
       "13  NURS  0.988235  0.987644  0.989873\n",
       "14  MEDI  0.989642  0.988009  0.990804\n",
       "15  DECI  0.989680  0.989050  0.991486\n",
       "16  VETE  0.992349  0.992143  0.993233\n",
       "17  ENGI  0.993466  0.992159  0.994520\n",
       "18  MATH  0.995436  0.994590  0.996427\n",
       "19  HEAL  0.996246  0.994962  0.997819\n",
       "20  COMP  0.996544  0.996474  0.996676\n",
       "21  PSYC  0.996987  0.995906  0.998350\n",
       "22  ECON  0.997437  0.996035  0.998860\n",
       "23  BUSI  0.997753  0.997017  0.999170\n",
       "24  ARTS  0.997935  0.997172  0.999480\n",
       "25  SOCI  0.997998  0.997556  0.999043"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_cosine_sim_df.sort_values(by=\"avg\", ignore_index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ewiser",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
