{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import spacy\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from itertools import chain\n",
    "from pathlib import Path\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constant paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to directory containing preprocessed COCA files\n",
    "COCA_PREPROC_DIR = Path(\"/Volumes/Elements/Capstone/coca-preproc-spacy/\")\n",
    "\n",
    "# Path to directory containing preprocessed Elsevier files\n",
    "ELSEVIER_PREPROC_DIR = Path(\"/Volumes/Elements/Capstone/elsevier-preproc-spacy/\")\n",
    "\n",
    "# Path to file containing all subject areas\n",
    "SUBJAREAS = Path(\"./subjareas.txt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Comparison Words\n",
    "First, we select 1000 tokens that are present in all corpora. These will be used to measure the distance between the general language corpus and the academic corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stopwords = nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tokens = set()\n",
    "\n",
    "with open(f'{COCA_PREPROC_DIR}/2015.pickle', 'rb') as f:\n",
    "    coca = pickle.load(f)\n",
    "\n",
    "    for text in coca:\n",
    "        text_tokens = [\n",
    "            token.text for sentence in text['text_docs'] for token in sentence\n",
    "            ]\n",
    "        filter_tokens = [\n",
    "            token.lower() for token in text_tokens if token.lower() not in stopwords\n",
    "            ]\n",
    "\n",
    "        unique_tokens.update(filter_tokens)\n",
    "\n",
    "with open(f'{SUBJAREAS}', 'r') as subj_list_file:\n",
    "    for subject in subj_list_file:\n",
    "        subject = subject.strip()\n",
    "\n",
    "        with open(f'{ELSEVIER_PREPROC_DIR}/{subject}.pickle', 'rb') as articles_file:\n",
    "            articles = pickle.load(articles_file)\n",
    "\n",
    "        unique_subj_tokens = set()\n",
    "\n",
    "        for text in articles:\n",
    "            text_tokens = [\n",
    "                token.text for sentence in text['body_text_docs'] for token in sentence if token.text\n",
    "                ]\n",
    "            filter_tokens = [\n",
    "                token.lower() for token in text_tokens if token.lower() not in stopwords\n",
    "                ]\n",
    "\n",
    "            unique_subj_tokens.update(filter_tokens)\n",
    "        \n",
    "        unique_tokens = unique_tokens.intersection(unique_subj_tokens)\n",
    "\n",
    "tokens_1000 = random.sample(unique_tokens, k=1000)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['demographic',\n",
       " 'database',\n",
       " 'discernible',\n",
       " 'sufficiently',\n",
       " 'nonetheless',\n",
       " 'assuming',\n",
       " 'irrelevant',\n",
       " 'let',\n",
       " 'ongoing',\n",
       " 'abstraction',\n",
       " 'landmarks',\n",
       " 'augmented',\n",
       " 'shear',\n",
       " 'helped',\n",
       " 'trends']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_1000 = random.sample(unique_tokens, k=1000)\n",
    "tokens_1000[:15]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COCA Embedding Space\n",
    "First we create an embedding space based on COCA, and save all the word vectors from the 1000 randomly selected tokens in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "coca_sentences = [\n",
    "    [token.text.lower() for token in sentence if token.text.lower() not in stopwords] \n",
    "    for text in coca for sentence in text['text_docs']\n",
    "    ]\n",
    "\n",
    "coca_model = Word2Vec(coca_sentences, window=5, min_count=1, workers=4)\n",
    "coca_token_vectors = [(token, coca_model.wv[token]) for token in tokens_1000]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ElSevier OA CC-BY Embedding Space\n",
    "Next, we create an embedding space for each academic corpus and compare the word vectors in the academic corpora with the word vectors in the COCA embedding space. This procedure is bootstrapped with 100 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subj_vector_sim(articles, vector_token_list):\n",
    "    \"\"\"\n",
    "    Given a set of articles and a list of word vectors, create a word2vec embedding space\n",
    "    based on articles and return the average cosine similarity between each word vector in \n",
    "    vector_token_list and their word vector in the new embedding space.\n",
    "\n",
    "    Args:\n",
    "    articles: Preprocessed articles from a specific discipline in the \n",
    "    ElSevier OA CC-BY corpus.\n",
    "    vector_token_list: A list of tuples. The item in the tuple contains a token, and the second \n",
    "    item in contains the word vector for that token in the COCA embedding space.\n",
    "    \"\"\"\n",
    "\n",
    "    subj_sentences = [\n",
    "        [token.text.lower() for token in sentence if token.text.lower() not in stopwords] \n",
    "        for text in articles for sentence in text['body_text_docs']\n",
    "        ]\n",
    "    \n",
    "    all_tokens = list(chain.from_iterable(subj_sentences))\n",
    "\n",
    "    subj_model = Word2Vec(subj_sentences, window=5, min_count=1, workers=4)\n",
    "\n",
    "    cosine_similarities = [\n",
    "        cosine_similarity(subj_model.wv[token[0]].reshape(1, -1), token[1].reshape(1, -1))\n",
    "        for token in vector_token_list if token[0] in all_tokens\n",
    "        ]\n",
    "\n",
    "    avg_cosine_similarity = np.mean(cosine_similarities)\n",
    "\n",
    "    return avg_cosine_similarity\n",
    "\n",
    "def bootstrap_word_embed_ci(\n",
    "        subj_articles,\n",
    "        word_vector_token_list, \n",
    "        num_resamples=100, \n",
    "        alpha=0.05, \n",
    "        ):\n",
    "    \"\"\"\n",
    "    Bootstrap the vector cosine similarity procedure to generate an average cosine similarity\n",
    "    with confidence intervals.\n",
    "\n",
    "    Args:\n",
    "    subj_articles: Preprocessed articles from a specific discipline in the \n",
    "    ElSevier OA CC-BY corpus.\n",
    "    word_vector_token_list: A list of tuples. The item in the tuple contains a token, and the second \n",
    "    item in contains the word vector for that token in the COCA embedding space.\n",
    "    num_resamples: The number of iterations for bootstrappign. Default = 100.\n",
    "    alpha: The alpha value for which to calculate the confidence intervals. Default = 0.05.\n",
    "    \"\"\"\n",
    "    average_cosine_sim = []\n",
    "    n = len(subj_articles)\n",
    "\n",
    "    for i in range(num_resamples):\n",
    "        resampled_texts = np.random.choice(subj_articles, size=n, replace=True)\n",
    "\n",
    "        sample_vector_sim = subj_vector_sim(resampled_texts, word_vector_token_list)\n",
    "\n",
    "        average_cosine_sim.append(sample_vector_sim)\n",
    "\n",
    "    average_sim = np.mean(average_cosine_sim)    \n",
    "    ci_lower = np.percentile(average_cosine_sim, alpha/2 * 100)\n",
    "    ci_upper = np.percentile(average_cosine_sim, (1 - alpha/2) * 100)\n",
    "\n",
    "    return average_sim, ci_lower, ci_upper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe to save results\n",
    "avg_cosine_sim_df = pd.DataFrame(columns=['subj', 'avg', 'lower CI', 'upper CI'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{ELSEVIER_PREPROC_DIR}/DENT.pickle', 'rb') as articles_file:\n",
    "    articles = pickle.load(articles_file)\n",
    "\n",
    "    bootstrap_word_vec_sim = bootstrap_word_embed_ci(articles, coca_token_vectors, num_resamples=1)\n",
    "    subject_row = {\n",
    "        'subj': 'DENT', 'avg': bootstrap_word_vec_sim[0], \n",
    "        'lower CI': bootstrap_word_vec_sim[1], 'upper CI': bootstrap_word_vec_sim[2]\n",
    "    }\n",
    "            \n",
    "    avg_cosine_sim_df = avg_cosine_sim_df.append(subject_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subj</th>\n",
       "      <th>avg</th>\n",
       "      <th>lower CI</th>\n",
       "      <th>upper CI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEM</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.973609</td>\n",
       "      <td>0.976081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CENG</td>\n",
       "      <td>0.975214</td>\n",
       "      <td>0.974769</td>\n",
       "      <td>0.976625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DENT</td>\n",
       "      <td>0.975412</td>\n",
       "      <td>0.975137</td>\n",
       "      <td>0.976084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PHYS</td>\n",
       "      <td>0.976533</td>\n",
       "      <td>0.976112</td>\n",
       "      <td>0.977410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PHAR</td>\n",
       "      <td>0.977424</td>\n",
       "      <td>0.976196</td>\n",
       "      <td>0.977934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AGRI</td>\n",
       "      <td>0.977696</td>\n",
       "      <td>0.977221</td>\n",
       "      <td>0.978083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>IMMU</td>\n",
       "      <td>0.978124</td>\n",
       "      <td>0.977284</td>\n",
       "      <td>0.979814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ENVI</td>\n",
       "      <td>0.979977</td>\n",
       "      <td>0.979750</td>\n",
       "      <td>0.981548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>BIOC</td>\n",
       "      <td>0.980124</td>\n",
       "      <td>0.978264</td>\n",
       "      <td>0.981100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NEUR</td>\n",
       "      <td>0.982154</td>\n",
       "      <td>0.980366</td>\n",
       "      <td>0.983440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>EART</td>\n",
       "      <td>0.984368</td>\n",
       "      <td>0.983190</td>\n",
       "      <td>0.985405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>MATE</td>\n",
       "      <td>0.986341</td>\n",
       "      <td>0.986323</td>\n",
       "      <td>0.987721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ENER</td>\n",
       "      <td>0.986897</td>\n",
       "      <td>0.985649</td>\n",
       "      <td>0.987831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NURS</td>\n",
       "      <td>0.988235</td>\n",
       "      <td>0.987644</td>\n",
       "      <td>0.989873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>MEDI</td>\n",
       "      <td>0.989642</td>\n",
       "      <td>0.988009</td>\n",
       "      <td>0.990804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>DECI</td>\n",
       "      <td>0.989680</td>\n",
       "      <td>0.989050</td>\n",
       "      <td>0.991486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>VETE</td>\n",
       "      <td>0.992349</td>\n",
       "      <td>0.992143</td>\n",
       "      <td>0.993233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ENGI</td>\n",
       "      <td>0.993466</td>\n",
       "      <td>0.992159</td>\n",
       "      <td>0.994520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>MATH</td>\n",
       "      <td>0.995436</td>\n",
       "      <td>0.994590</td>\n",
       "      <td>0.996427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>HEAL</td>\n",
       "      <td>0.996246</td>\n",
       "      <td>0.994962</td>\n",
       "      <td>0.997819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>COMP</td>\n",
       "      <td>0.996544</td>\n",
       "      <td>0.996474</td>\n",
       "      <td>0.996676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>PSYC</td>\n",
       "      <td>0.996987</td>\n",
       "      <td>0.995906</td>\n",
       "      <td>0.998350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ECON</td>\n",
       "      <td>0.997437</td>\n",
       "      <td>0.996035</td>\n",
       "      <td>0.998860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>BUSI</td>\n",
       "      <td>0.997753</td>\n",
       "      <td>0.997017</td>\n",
       "      <td>0.999170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>SOCI</td>\n",
       "      <td>0.997888</td>\n",
       "      <td>0.997556</td>\n",
       "      <td>0.999043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ARTS</td>\n",
       "      <td>0.997935</td>\n",
       "      <td>0.997172</td>\n",
       "      <td>0.999480</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    subj       avg  lower CI  upper CI\n",
       "0   CHEM  0.975000  0.973609  0.976081\n",
       "1   CENG  0.975214  0.974769  0.976625\n",
       "2   DENT  0.975412  0.975137  0.976084\n",
       "3   PHYS  0.976533  0.976112  0.977410\n",
       "4   PHAR  0.977424  0.976196  0.977934\n",
       "5   AGRI  0.977696  0.977221  0.978083\n",
       "6   IMMU  0.978124  0.977284  0.979814\n",
       "7   ENVI  0.979977  0.979750  0.981548\n",
       "8   BIOC  0.980124  0.978264  0.981100\n",
       "9   NEUR  0.982154  0.980366  0.983440\n",
       "10  EART  0.984368  0.983190  0.985405\n",
       "11  MATE  0.986341  0.986323  0.987721\n",
       "12  ENER  0.986897  0.985649  0.987831\n",
       "13  NURS  0.988235  0.987644  0.989873\n",
       "14  MEDI  0.989642  0.988009  0.990804\n",
       "15  DECI  0.989680  0.989050  0.991486\n",
       "16  VETE  0.992349  0.992143  0.993233\n",
       "17  ENGI  0.993466  0.992159  0.994520\n",
       "18  MATH  0.995436  0.994590  0.996427\n",
       "19  HEAL  0.996246  0.994962  0.997819\n",
       "20  COMP  0.996544  0.996474  0.996676\n",
       "21  PSYC  0.996987  0.995906  0.998350\n",
       "22  ECON  0.997437  0.996035  0.998860\n",
       "23  BUSI  0.997753  0.997017  0.999170\n",
       "25  SOCI  0.997888  0.997556  0.999043\n",
       "24  ARTS  0.997935  0.997172  0.999480"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_cosine_sim_df.sort_values(by=\"avg\", ignore_index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ewiser",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
